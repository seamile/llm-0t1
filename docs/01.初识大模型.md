# 一、初识大模型

![LLM](assets/llm-brain.jpg)

2022 年底，随着 ChatGPT 3.5 横空出世，“大模型”这一概念迅速引爆科技圈，进入公众视野，成为全民热议的话题。

## 1. 什么是大模型

如今我们常说的 “大模型” 一般是指 “大语言模型” (英文：Large Language Model，常缩写为 LLM)。它是一种在自然语言处理领域采用超大规模参数和海量数据训练而成的深度神经网络模型。

大语言模型最初的设计目的是为了提高模型的表达能力和预测性能，它本质上只是在预测句子中的下一个单词是什么。但经过足够数据量的训练后人们发现这种语言模型可以捕获人类语言的句法和语义，并且能够在训练期间“记住”大量事实。这赋予了大模型更强大的能力，相较于传统的机器学习它能够处理更加复杂的数据和任务。

传统机器学习模型的参数数量通常在百万级到千万级，而大模型则往往拥有数十亿甚至数千亿个参数。为了获得如此大规模的参数，需要利用互联网中海量的文本、图像、视频等非结构化数据来进行预训练，数据集往往高达数十 TB 甚至数百 TB 的规模。
<div align="center"><img alt="machine_learning" src="assets/machine_learning.png" width="85%" /></div>

与传统的专门为某一类任务设计的模型不同，大模型在预训练阶段习得了广泛的知识，让它具有很强的通用性、泛化能力、以及涌现能力，可以应用于多种任务，比如：文章创作、翻译、编程等等。

## 2. 大模型的前世今生

### 2.1 早期阶段：统计语言模型

大模型的发展历史可以追溯到20世纪90年代。在深度学习兴起之前，自然语言处理主要依赖于统计学方法。N-gram 模型是这一时期常用的语言模型，通过计算词序列的概率来预测下一个词。然而，统计语言模型在处理长距离依赖和稀疏数据方面表现较差。

<div align="center"><img alt="N-gram" src="assets/N-gram.webp" width="85%" /></div>


2003年，深度学习领域最有影响力的先驱之一 Bengio 提出了第一个神经语言模型，将词嵌入（Word Embedding）引入到语言建模中。这一模型利用神经网络来捕捉词之间的关系，提高了语言模型的性能。

### 2.2 深度学习阶段

2010年代初，循环神经网络（RNN）和长短期记忆网络（LSTM）开始应用于语言模型。这些模型能够处理变长序列数据，并捕捉长距离依赖，但由于梯度消失问题，其性能受到一定限制。

2013年，Google 的 Tomas Mikolov 等人提出了著名的 [Word2vec 模型](https://arxiv.org/abs/1301.3781)。这一模型基于大规模语料库训练的单词嵌入,它使得词汇的连续向量表征成为可能。Word2vec 有效解决了 N-gram 模型中 "curse of dimensionality" 的瓶颈。

<div align="center"><img alt="Word2vec" src="assets/Word2vec.webp" width="85%" /></div>

2014年，Ilya Sutskever 与谷歌研究员 Oriol Vinyals 和 Quoc Le 一起提出了 [Seq2Seq 模型](https://arxiv.org/abs/1409.3215)，并将之用于机器翻译。随后，Bahdanau 等人引入了注意力机制，解决了长序列翻译中的信息压缩问题，大大提升了模型性能。

<div align="center"><img alt="Seq2Seq" src="assets/seq2seq.png" width="85%" /></div>

### 2.3 Transformer 架构的引入

前期的这些探索，有成功又失败，它们共同奠定了大语言模型的技术基石。

2017年，由Ashish Vaswani等人撰写的论文[《Attention Is All You Need》](https://arxiv.org/abs/1706.03762) 中首次介绍了Transformer模型。这篇论文由Google的团队发表，它提出了一种完全基于注意力机制的全新架构，而不再依赖于循环神经网络（RNN）或卷积神经网络（CNN），这种模型能够高效并行地处理序列数据，极大提高了模型的训练效率和效果。

Transformer 是一项重大突破，它发布标志着自然语言处理的进入了一个全新阶段。

<div align="center"><img alt="Transformer" src="assets/transformer.png" width="600" /></div>

2018年6月，OpenAI发布了首个基于Transformer架构的GPT（Generative Pre-trained Transformer）模型，该模型拥有 1.17 亿的参数量，它展示了预训练和微调（fine-tuning）在语言模型中的巨大潜力。GPT模型通过大规模预训练，然后在特定任务上进行微调，实现了卓越的性能。

同一年 10 月，Google发布了BERT（Bidirectional Encoder Representations from Transformers）模型，该模型同样基于 Transformer 并采用双向编码器架构，能够同时考虑上下文信息。BERT在多个NLP基准任务上取得了前所未有的高分。

![BERT-and-GPT](assets/bert-and-gpt.png)

2019年，OpenAI发布了GPT-2模型，具有15亿参数，展示了出色的自然语言文本的生成能力。2020年，OpenAI进一步发布了GPT-3模型，用来训练的语料库积累了 8 年，涵盖了几乎所有可用的互联网数据源，其参数量达到了惊人的 1750亿。

GPT-3.0 在多种语言任务上表现卓越，还展现出了一些在较小模型中未曾观察到的能力，比如更好的语言理解、更复杂的任务处理能力、令人惊叹的涌现能力、以及在零样本和少样本学习中的出色表现。GPT-3.0 模型展现了使用超大规模语料库进行预训练对语言理解和生成任务的价值，它是GPT系列中迄今为止最著名的版本，它促成了当前大语言模型百花齐放的局面。

![LLM进化树](assets/llm-models-tree.jpg)

在随后的研究中，全球众多公司、科研机构纷纷推出了更大规模的模型，相关的算法被不断改进，大模型的能力被充分挖掘，应用场景也不断拓展。研究人员开始探索将多种数据类型（如文本、图像、音频等）结合在一起的大模型。例如，OpenAI的CLIP模型、DALL-E模型、Sora 模型，将文本、图像、视频结合，展示了强大的跨模态生成和理解能力。


## 3. 国内外大模型概览

大模型发展至今，已经不胜枚举，他们的技术栈大体都很相似，其形式基本也都是以聊天对话为主，我们没有必要对他们一一尝试。

下表中列举一些比较有代表性的大模型：

| 名称                                                   | 所属公司               |           是否开源           |
|--------------------------------------------------------|------------------------|:----------------------------:|
| [ChatGPT](https://chatgpt.com/)                        | OpenAI                 |  <font color="red">✗</font>  |
| [Claude](https://claude.ai/)                           | Anthropic              |  <font color="red">✗</font>  |
| [Copilot](https://copilot.microsoft.com/)              | Microsoft              |  <font color="red">✗</font>  |
| [Gemini](https://gemini.google.com/)                   | Google                 |  <font color="red">✗</font>  |
| [Llama3](https://www.meta.ai/)                         | Meta (Facebook)        | <font color="green">✓</font> |
| [Mixtral](https://mistral.ai/)                         | Mistral AI             | <font color="green">✓</font> |
| [ChatGLM](https://chatglm.cn/)                         | 智谱AI                 | <font color="green">✓</font> |
| [通义千问](https://tongyi.aliyun.com/)                 | 阿里巴巴               | <font color="green">✓</font> |
| [Kimi](https://kimi.moonshot.cn/)                      | 月之暗面 (Moonshot AI) |  <font color="red">✗</font>  |
| [DeepSeek](https://chat.deepseek.com/)                 | 深度求索               | <font color="green">✓</font> |
| [百川](https://www.baichuan-ai.com/)                   | 百川智能               | <font color="green">✓</font> |
| [豆包](https://www.doubao.com/chat/)                   | 字节跳动               |  <font color="red">✗</font>  |
| [万知](https://www.wanzhi.com/)                        | 零一万物               | <font color="green">✓</font> |
| [文心一言](https://yiyan.baidu.com/)                   | 百度                   |  <font color="red">✗</font>  |
| [星火](https://xinghuo.xfyun.cn/)                      | 科大讯飞               |  <font color="red">✗</font>  |
| [元宝](https://yuanbao.tencent.com/)                   | 腾讯                   | <font color="green">✓</font> |
| [盘古](https://www.huaweicloud.com/product/pangu.html) | 华为                   |  <font color="red">✗</font>  |

在大模型领域，OpenAI的GPT系列无疑占据着举足轻重的地位，每一次新版本的发布都能吸引行业内的广泛关注，并 数次引领人工智能发展的新趋势。

Claude、Gemini (Bard)、Copilot (Bing Chat) 等模型在早期便已崭露头角且表现出色，但Meta的Llama系列因开源的特性，在开发者社区中占据着独特的地位。不仅有很多机构和企业以 Llama3 为基座开发出了自己的大模型，而且它还提供仅80亿参数的轻量化版本，使得个人用户也能在普通计算机上部署和调试，成为目前最受欢迎的开源大模型之一。

在国内，百度的“文心一言”作为先行者，其综合能力直逼国际顶尖水平，稳居行业第一梯队。然而在开发者群体中，反响更大的则是智谱AI的ChatGLM和阿里巴巴的“通义千问”大模型，两者同样采取了开源策略。

ChatGLM是由清华大学的KEG实验室与智谱AI合作开发，紧随“文心一言”之后发布，其推理性能在诸多大语言模型中名列前茅。

另一款值得关注的大模型是月之暗面（Moonshot AI）的Kimi，它支持超长文本的输入与输出，最大可达200万字符，为用户提供了非常不错的交互体验。

此外，DeepSeek凭借其独特的MoE与MLA（假设的多层架构）结合的架构设计，实现了在保证模型效能的同时，大幅降低了推理成本。DeepSeek平台率先将百万Token的价格降至1元。其V2版本发布时，在代码生成能力上更是取得了与GPT-4相媲美的成绩。

我们后面教程中将要使用的大模型主要是 ChatGPT、Llama3、ChatGLM、Qwen。


## 4. 大模型价格对比

随着算法的不断优化以及硬件性能的提升，大模型的训练成本和推理成本也不断降低。2024 年 5 月，国内外的大模型经历了一轮史无前例的大降价，有的比原来降低了 90% 多，部分模型甚至直接可以免费使用。

这极大的降低了我们作为普通开发者的使用门槛。下表中是每 100 万 Token 的价格表，数据日期 2024-06-20。

| 厂商      | 模型                  | 上下文长度 | 输入   | 输出   |
|-----------|-----------------------|------------|--------|--------|
| 深度求索  | deepseek-chat         | 32k        | ¥ 1    | ¥ 2    |
|           | deepseek-coder        | 32k        | ¥ 1    | ¥ 2    |
| 智谱清言  | GLM-4-Flash           | 128k       | ¥ 0.1  | ¥ 0.1  |
|           | GLM-4-Air             | 128k       | ¥ 1    | ¥ 1    |
|           | GLM-4-AirX            | 8k         | ¥ 10   | ¥ 10   |
|           | GLM-4V                | 2k         | ¥ 50   | ¥ 50   |
|           | GLM-4-0520            | 128k       | ¥ 100  | ¥ 100  |
| 腾讯      | hunyuan-lite          | 256K       | ¥ 0    | ¥ 0    |
|           | hunyuan-standard      | 32K        | ¥ 4.5  | ¥ 5    |
|           | hunyuan-standard-256k | 256K       | ¥ 15   | ¥ 60   |
|           | hunyuan-pro           | 32K        | ¥ 30   | ¥ 100  |
| 阿里巴巴  | qwen-long             | 1000万     | ¥ 0.5  | ¥ 2    |
|           | qwen-turbo            | 8K         | ¥ 2    | ¥ 6    |
|           | qwen-plus             | 32K        | ¥ 4    | ¥ 12   |
|           | qwen-max              | 8K         | ¥ 40   | ¥ 120  |
| 字节跳动  | Doubao-pro-4k         | 4K         | ¥ 0.8  | ¥ 2    |
|           | Doubao-pro-32k        | 32K        | ¥ 0.8  | ¥ 2    |
|           | Doubao-pro-128k       | 128K       | ¥ 5    | ¥ 9    |
| 月之暗面  | moonshot-v1-8k        | 8K         | ¥ 12   | ¥ 12   |
|           | moonshot-v1-32k       | 32K        | ¥ 24   | ¥ 24   |
|           | moonshot-v1-128k      | 128K       | ¥ 60   | ¥ 60   |
| 百度      | ERNIE-Speed           | 8K、128K   | ¥ 免费 | ¥ 免费 |
|           | ERNIE-Lite            | 8K、128K   | ¥ 免费 | ¥ 免费 |
|           | ERNIE-3.5系列         | 8K         | ¥ 12   | ¥ 12   |
|           | ERNIE-4.0系列         | 8K         | ¥ 120  | ¥ 120  |
| 零一万物  | yi-spark              | 16K        | ¥ 1    | ¥ 1    |
|           | yi-medium             | 16K        | ¥ 2.5  | ¥ 2.5  |
|           | yi-large              | 32K        | ¥ 20   | ¥ 20   |
| OpenAI    | gpt-3.5-turbo-0125    | 16K        | $ 0.5  | $ 1.5  |
|           | gpt-4o                | 128K       | $ 5    | $ 15   |
|           | gpt-4-turbo           | 128K       | $ 10   | $ 30   |
|           | gpt-4                 | 多模态     | $ 30   | $ 60   |
| Google    | Gemini 1.5 Pro        | 128K       | $ 3.5  | $ 10.5 |
|           | Gemini 1.0 Pro        | 32K        | $ 0.5  | $ 1.5  |
| Anthropic | Claude 3 Haiku        | 200K       | $ 0.25 | $ 1.25 |
|           | Claude 3 Sonnet       | 200K       | $ 3    | $ 15   |
|           | Claude 3 Opus         | 200K       | $ 15   | $ 75   |
